{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<a href=\"https://colab.research.google.com/github/SAIIL/CVS_challenge_code/blob/master/notebooks/cvs_challenge_sample_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "xscpCWkNyRha"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00AM-dwUYUPD"
      },
      "source": [
        "<div>\n",
        "<a href=\"https://cvs-challenge.grand-challenge.org/\">\n",
        "<img src=\"https://rumc-gcorg-p-public.s3.amazonaws.com/b/652/CVS_Challenge_Media_-_Summit_Ad_3.x20.jpeg\" align=\"left\"/>\n",
        "</a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOoy9D5tYbBC"
      },
      "source": [
        "## <h1><center>SAGES CVS Challenge Sample Code</center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we provide sample code to help you **familiarize yourself with the SAGES CVS challenge, visualize the dataset, and explore the metrics**. The notebook offers a minimal example of a simple deep learning pipeline to be applied on a small subset of the cvs challenge dataset before using the entire CVS challenge dataset.\n",
        "\n",
        "The notebook contains step by step instructions to:\n",
        "\n",
        "1.  Load & visualize the CVS Challenge data.\n",
        "2.  Build & run a model for a multi-label classification task.\n",
        "3.  Predict the three CVS criteria (C1, C2, and C3).\n",
        "4.  Evaluate your model performance using the CVS challenge metrics.\n",
        "\n",
        "**Important Notice:**\n",
        "For more information on the SAGES CVS Challenge visit https://cvs-challenge.grand-challenge.org/ . Access to the SAGES CVS Challenge dataset is only provided to registered participants, who have signed the SAGES CVS Challenge Participation Agreement.\n"
      ],
      "metadata": {
        "id": "YV89uIR70l7G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohEpKbgTYD-K"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGXqgKOzYD-M"
      },
      "source": [
        "\n",
        "**Instructions for initial Demonstration:**\n",
        "* For demonstration purposes of this notebook, we have added a zip file containing 3 sample videos of the CVS Challenge dataset.\n",
        "* Run the notebook to verify you get a result file in the format of `results.json`.\n",
        "* Run this colab with the provided zip file first, before attempting the entire CVS Challenge dataset on your machine.\n",
        "\n",
        "**Data Structure**\n",
        "* The provided zip file (`cvs_challenge_examples.zip`), here referred to as `data/`, contains subfoldes named:\n",
        "* 'videos/' - containing 3 sample videos of the CVS Challenge dataset in the format `<video_filename>.mp4`\n",
        "* 'labels/' - containing the annotations of the 3 sample videos of the CVS Challenge dataset in the format `<video_filename>/frame.csv`,`<video_filename>/video.csv`\n",
        "\n",
        "\n",
        "**Instructions for actual CVS Challenge Competition:**\n",
        "* When running on the full CVS challenge dataset, you will need to replace the `data/` folder in the colab with anew folder/location containing the entire CVS Challenge data (provided via Dropbox after successful registration).\n",
        "* The data structure of the full CVS Challenge dataset remains the same as in the zip file (see above and README provided with full dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YR5f4qbYD-N"
      },
      "source": [
        "# 1 - Data Loading and Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we demonstrate how to mount the colab and data, visualize the video frames and corresponding labels. For detailed instructions on the dataset and labels please visit https://cvs-challenge.grand-challenge.org/dataset/."
      ],
      "metadata": {
        "id": "pOsFOibZvGmr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLZd424QZnq0"
      },
      "outputs": [],
      "source": [
        "!pip install -q timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-k2nZN-b-te"
      },
      "outputs": [],
      "source": [
        "# Mount a location on your GoogleDrive to store your colab and data.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xetC8Dicr1u"
      },
      "outputs": [],
      "source": [
        "# Load the sample code from the CVSchallenge-repository (located on GitHub) to your colab.\n",
        "!mkdir /root/.ssh\n",
        "\n",
        "# Connect the mounted GoogleDrive link with the CVSchallenge-repository GitHub deploy key. Copy the GitHub key from\n",
        "# https://drive.google.com/file/d/1xqUkSBlQ6h_kB-k2Pmo2qQMCB24xXCwx/view?usp=sharing, into your google drive, under cvs_colab_key, before running the colab.\n",
        "!cp /content/drive/MyDrive/cvs_colab_key /root/.ssh/id_ed25519\n",
        "!ssh-keyscan github.com >> /root/.ssh/known_hosts\n",
        "# Using the GitHub key, download the CVS challenge code from the repository.\n",
        "!rm -Rf CVS_challenge_code\n",
        "!git clone git@github.com:SAIIL/CVS_challenge_code.git\n",
        "\n",
        "# Add the GitHub code folder into the python path\n",
        "import sys\n",
        "sys.path.insert(0,'/content/CVS_challenge_code')\n",
        "\n",
        "# Copy the data into the data directory\n",
        "!mkdir ./data\n",
        "\n",
        "# Important Notice: This currently points to a Dropbox link containing 3 sample videos from the CVS Challenge Dataset (1st Batch) for demo purposes.\n",
        "# For the actual CVS Challenge competition, you will need to replace this data source\n",
        "# with the full CVS Challenge training data (access after successful registration on https://cvs-challenge.grand-challenge.org/).\n",
        "# You can either replace this Dropbox link with the link to the CVS Challenge training data or specify a local folder.\n",
        "\n",
        "!wget \"https://www.dropbox.com/scl/fi/pnq4i330w3lnbju2uszwv/cvs_challenge_examples.zip?rlkey=qtusk9h03ccnf9m7ecxi8h2pu&dl=0\" -O ./cvs_challenge_examples.zip\n",
        "!unzip ./cvs_challenge_examples.zip -d ./data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaD-0xD6YD-N"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries.\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import random\n",
        "from statistics import mode\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import accuracy_score, f1_score, average_precision_score, brier_score_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf13RdaLaCL0"
      },
      "outputs": [],
      "source": [
        "# Run a bash shell script to extract frames from videos.\n",
        "# Note: The data is now located in /content/data in your colab. If you moved the data to a different directory,\n",
        "# change the dataset path for the video_folder, frame_folder variables below, from 'data/' to the new path.\n",
        "\n",
        "%%shell\n",
        "\n",
        "video_folder=\"data/videos\"\n",
        "frame_folder=\"data/frames\"\n",
        "mkdir -p \"$frame_folder\"\n",
        "\n",
        "video_count=0\n",
        "\n",
        "# Extract the video frames and save.\n",
        "for video_file in \"$video_folder\"/*.mp4; do\n",
        "\n",
        "    base_name=$(basename \"$video_file\" .mp4)\n",
        "    mkdir -p \"$frame_folder/$base_name\"\n",
        "\n",
        "    ffmpeg -i \"$video_file\" -vf fps=30 -q:v 2 \"$frame_folder/$base_name/%04d.jpg\"\n",
        "\n",
        "    # Break after 3rd video for demonstration, to save space.\n",
        "    # For the actual CVS Challenge Competition / when using the full training data remove the break.\n",
        "    video_count=$((video_count + 1))\n",
        "    if [ \"$video_count\" -ge 3 ]; then\n",
        "        break\n",
        "    fi\n",
        "done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow24HZoUYD-O"
      },
      "outputs": [],
      "source": [
        "# Visualize the 18 annotated frames in each video and corresponding annotations.\n",
        "\n",
        "video_folder = 'data/videos'\n",
        "frames_path = 'data/frames'\n",
        "labels_path = 'data/labels'\n",
        "\n",
        "print(os.listdir(video_folder))\n",
        "video_names = [os.path.splitext(video_file)[0] for video_file in os.listdir(video_folder) if os.path.isdir(os.path.join(frames_path, os.path.splitext(video_file)[0]))]\n",
        "video_name = random.choice(video_names)\n",
        "print('Video:', video_name)\n",
        "\n",
        "label_fname = os.path.join(labels_path, f\"{video_name}/frame.csv\")\n",
        "label = pd.read_csv(label_fname)\n",
        "\n",
        "# Print the \"majority vote annotation\" resulting from the 3 raters (annotators), which serves as the ground truth label.\n",
        "def majority_vote(row, category):\n",
        "    labels = [row[f\"{category}_rater1\"], row[f\"{category}_rater2\"], row[f\"{category}_rater3\"]]\n",
        "    return mode(labels)\n",
        "\n",
        "label['c1_majority'] = label.apply(majority_vote, category='c1', axis=1)\n",
        "label['c2_majority'] = label.apply(majority_vote, category='c2', axis=1)\n",
        "label['c3_majority'] = label.apply(majority_vote, category='c3', axis=1)\n",
        "label.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itzq5fZ2YD-P"
      },
      "outputs": [],
      "source": [
        "for index, row in label.iterrows():\n",
        "    frame_id = row['frame_id']\n",
        "    c1,c2,c3 = row['c1_majority'],row['c2_majority'],row['c3_majority']\n",
        "    frame_path = os.path.join(frames_path, video_name, f\"{frame_id+1:04d}.jpg\")  # Assuming frame files are in PNG format\n",
        "\n",
        "    img = Image.open(frame_path)\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Frame ID: {frame_id}, Label: {c1,c2,c3}\")\n",
        "    plt.show()\n",
        "    plt.pause(0.1)\n",
        "    clear_output(wait=True)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRR23KmCYD-Q"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import datasets.cvs_datasets\n",
        "from importlib import reload\n",
        "reload(datasets)\n",
        "reload(datasets.cvs_datasets)\n",
        "from datasets.cvs_datasets import CVSData\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "frames_path = 'data/frames'\n",
        "labels_path = 'data/labels'\n",
        "dataset = CVSData(frames_path=frames_path, labels_path=labels_path, transform=transform)\n",
        "\n",
        "img, label, video_name, frame_id, metadata = dataset[0]\n",
        "print(f'Video {video_name}, frame {frame_id}')\n",
        "print('Label of c1 c2 c3:', label)\n",
        "print(f'Confidence aware labels: {metadata[\"confidence_aware_labels\"]}')\n",
        "image_width, image_height = img.shape[2], img.shape[1]\n",
        "\n",
        "plt.imshow(img.permute(1,2,0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMM6T_aYYD-Q"
      },
      "source": [
        "# 2 - Building & Running Multi-class Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73nxwHvPYD-Q"
      },
      "source": [
        "In this section we demonstrate how to\n",
        "1. Build a model for multi label classification task.\n",
        "2. Perform a simple forward pass of a PyTorch batch input using a ViT based model.\n",
        "3. Predict the class labels corresponding to the frames.\n",
        "\n",
        "In the CVS Challenge there are **three binary labels** corresponding to the three CVS criteria C1, C2 and C3. For more information on the dataset & labels visit https://cvs-challenge.grand-challenge.org/dataset/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gw3WNCV4YD-Q"
      },
      "outputs": [],
      "source": [
        "# Define a model for multi label classification.\n",
        "\n",
        "class ViTMultiLabelClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=3, dropout=0.0, pretrained=True):\n",
        "        super(ViTMultiLabelClassifier, self).__init__()\n",
        "        # Pre-trained ViT as backbone\n",
        "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=pretrained)\n",
        "        self.vit.head = nn.Identity()\n",
        "\n",
        "        # MLP as classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.vit.embed_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
        "        \"\"\" Classify images\n",
        "        Args:\n",
        "          x (torch.Tensor): The input images, of size BxCxHxW.\n",
        "\n",
        "        Returns:\n",
        "          torch.Tensor: a Bx3 vector specifying the confidence for each label (C1,C2,C3 of the CVS)\n",
        "        \"\"\"\n",
        "        x = self.vit(x)  # Pass input image to ViT\n",
        "        x = self.classifier(x)  # Pass through MLP classifier\n",
        "        return torch.sigmoid(x)  # Use sigmoid for multi-label classification\n",
        "\n",
        "# Example of use of the class\n",
        "if __name__ == \"__main__\":\n",
        "    model = ViTMultiLabelClassifier()\n",
        "    input_tensor = torch.rand((1, 3, 224, 224))\n",
        "\n",
        "    outputs = model(input_tensor)\n",
        "    print(\"Model outputs:\", outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1lZwPTLYD-Q"
      },
      "source": [
        "# 3 - Prediction of the CVS Criteria"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we create a dataloader and demonstrate the target format for challenge submissions, by running the model on the data examples, aggregating results over the dataset, and then saving the result.json file."
      ],
      "metadata": {
        "id": "-zAaugQ1v_Fg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AffT7mYLYD-R"
      },
      "outputs": [],
      "source": [
        "from util.util import save_results,load_results\n",
        "\n",
        "# Create dataset and dataloader.\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "frames_path = 'data/frames'\n",
        "labels_path = 'data/labels'\n",
        "\n",
        "# This is a demo PyTorch dataset to be used within the challenge, loading the 3 sample videos provided for demo purposes.\n",
        "dataset = CVSData(frames_path=frames_path, labels_path=labels_path, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Create model instance\n",
        "model = ViTMultiLabelClassifier()\n",
        "\n",
        "# Run the classifier model, estimate the label confidences over the whole dataset.\n",
        "# Save the labels, confidences from both ground truth and model results.\n",
        "\n",
        "overall_outputs=[]\n",
        "overall_predictions=[]\n",
        "overall_labels=[]\n",
        "overall_confidence_aware_labels=[]\n",
        "overall_raw_labels={\n",
        "                    'c1':[],\n",
        "                    'c2':[],\n",
        "                    'c3':[]\n",
        "                    }\n",
        "for img, label, video_name, frame_id, metadata in dataloader:\n",
        "    output = model(img)\n",
        "    prediction = (output > 0.5).float()\n",
        "    overall_labels.append(label.cpu().numpy())\n",
        "    overall_confidence_aware_labels.append(np.transpose(np.array([metadata['confidence_aware_labels']['c1'],\n",
        "                                            metadata['confidence_aware_labels']['c2'],\n",
        "                                            metadata['confidence_aware_labels']['c3']])))\n",
        "    for crit in ['c1','c2','c3']:\n",
        "      overall_raw_labels[crit].append(np.concatenate(np.expand_dims(metadata['raw_labels'][crit],0),1))\n",
        "    overall_predictions.append(prediction.detach().cpu().numpy())\n",
        "    overall_outputs.append(output.detach().cpu().numpy())\n",
        "\n",
        "# Plot over the last example\n",
        "batch_size = img.shape[0]\n",
        "for i in range(batch_size):\n",
        "    plt.imshow(img[i].permute(1, 2, 0))\n",
        "    # Convert from (C, H, W) to (H, W, C) for plotting\n",
        "    plt.title(f\"Vid: {video_name[i]}\\nFrame ID: {frame_id[i]}\")\n",
        "    plt.axis('off')\n",
        "    # Hide axes\n",
        "    plt.show()\n",
        "\n",
        "for crit in ['c1','c2','c3']:\n",
        "    overall_raw_labels[crit]=np.transpose(np.concatenate(overall_raw_labels[crit],1))\n",
        "overall_predictions=np.concatenate(overall_predictions)\n",
        "# N_samples x 3, values are 0 or 1\n",
        "overall_labels=np.concatenate(overall_labels)\n",
        "# Ground truth values, N_samples x 3, values are 0 or 1\n",
        "overall_outputs=np.concatenate(overall_outputs)\n",
        "# N_samples x 3, values are continuous between 0 and 1\n",
        "overall_confidence_aware_labels = np.concatenate(overall_confidence_aware_labels)\n",
        "\n",
        "# Save results file.\n",
        "save_results(overall_predictions,overall_outputs,'result.json')\n",
        "\n",
        "dataset_results=load_results('result.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Evaluation using CVS Challenge Metrics"
      ],
      "metadata": {
        "id": "PMnTdsFxwqcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we demonstrate how to compute the CVS Challenge metrics. For more information please visit https://cvs-challenge.grand-challenge.org/evaluation1/  (Access only after successful challenge registration).\n",
        "\n",
        "\n",
        "*   For **Subchallenge A**, we are looking for models with the highest Mean Average Precision (mAP) on CVS detection.\n",
        "*   For **Subchallenge B**, we are looking for predictive models that demonstrate “awareness” of the inherent uncertainty represented in a given image,\n",
        "both in terms of the subjectivity of the task (CVS classification) and complexity of the case being annotated.\n",
        "We will calculate the uncertainty of the Brier Score (BS).\n",
        "*   For **Subchallenge C**, we are looking for the model robustness to variations in distribution shifts. These can be linked to clinical characteristics or technical attributes of the data.\n",
        "\n",
        "We will evaluate submissions on the held out 300 testing videos. For subchallenge C we will use several testing on variant datasets  (resamples from the 300 testing videos with undisclosed distribution shifts) to examine robustness to deployment-time distribution shifts. These shifts will be based on the demographic and clinical metadata (for more details visit https://cvs-challenge.grand-challenge.org/dataset/).\n",
        "\n",
        "Performance will be calculated over all images in the dataset and then averaged across the CVS criteria for each of the variant test sets. We will pick the minimum average precisions across all these variant splits, ignoring the bottom 90th percentile to account for outliers."
      ],
      "metadata": {
        "id": "waQu2tOWxA1a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx5HKmshikGk"
      },
      "outputs": [],
      "source": [
        "from util.metrics import compute_overall_metrics\n",
        "'''\n",
        "Evaluate metrics\n",
        "'''\n",
        "\n",
        "# Use the results from the saved json.\n",
        "overall_predictions2 = dataset_results['estimated_detected_labels']\n",
        "overall_outputs2 = dataset_results['estimated_label_confidences']\n",
        "\n",
        "# Print the dataset/result items and their sizes\n",
        "print(f'Prediction of c1 c2 c3: {overall_predictions}')\n",
        "print(overall_raw_labels['c1'])\n",
        "print(overall_confidence_aware_labels)\n",
        "print(len(overall_raw_labels['c1']))\n",
        "print(len(overall_predictions2[:,i]))\n",
        "\n",
        "# Run metrics computation\n",
        "metrics = compute_overall_metrics(overall_labels, overall_confidence_aware_labels, overall_outputs2)\n",
        "\n",
        "# Extract the metrics from the result of the metrics computation\n",
        "accuracy = metrics['accuracy']\n",
        "f1 = metrics['f1']\n",
        "mAP = metrics['mAP']\n",
        "\n",
        "# Print the results of the metrics computation\n",
        "print(f'Label of c1 c2 c3: {overall_labels}')\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"mAP Score: {mAP}\")\n",
        "\n",
        "for i,key in enumerate(['c1','c2','c3']):\n",
        "  brier_score = metrics['brier_score'][key]\n",
        "  print(f\"Brier score for c{i}: {brier_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jbAP0wZ5TPE"
      },
      "outputs": [],
      "source": [
        "# Print the metadata, frame ID, and the raw labels for criteria 1, as an example.\n",
        "print(metadata)\n",
        "print(frame_id)\n",
        "print(np.concatenate(np.expand_dims(metadata['raw_labels']['c1'],0),1))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}